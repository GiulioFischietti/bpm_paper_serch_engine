{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Get all the articles to summarize","metadata":{"id":"DxGk5T1ntmeD"}},{"cell_type":"code","source":"import json\nimport requests\n\nhost = \"https://0adf-82-50-143-221.ngrok-free.app\"\narticles_json = requests.get(f\"{host}/articlesToSummarize\").json()","metadata":{"id":"U3QaK6LBtmeE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Instanciate BART","metadata":{"id":"sRL-nBuotmeE"}},{"cell_type":"code","source":"from transformers import BartForConditionalGeneration, AutoTokenizer\n\nmodel_ckpt = \"sshleifer/distilbart-cnn-6-6\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = BartForConditionalGeneration.from_pretrained(model_ckpt).cuda()","metadata":{"id":"XaiIbmpItmeE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check Cuda comparibility","metadata":{"id":"epITcYjKtmeF"}},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nimport math\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kJVuXRpTtmeF","outputId":"6ee83d7a-99a4-4e52-992d-0b1b437f3c50","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize and filter by number of pages\nIf a paper has more than 10 pages, it is filtered out as a single summary will be too long for chat gpt input\n","metadata":{}},{"cell_type":"code","source":"page_size = 1024\n\npaper_rows = []\nskipped_papers = []\nfor paper in tqdm(articles_json):\n    tokens = tokenizer(paper['body'], padding='max_length', return_tensors='pt').to(device)\n    n_pages = math.ceil(len(tokens[\"input_ids\"][0])/page_size)\n    paper_rows.append(\n            {\n            \"link\": paper[\"link\"],\n            \"body\":  paper[\"body\"],\n            \"summary\":  \"\",\n            \"input_ids\":  tokens\n            }\n        )","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uc8eLs1ptmeF","outputId":"04c74cd2-3fc6-4043-ad1d-ec5bf2fb1776","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summarize Docs\nSplit tokens in pages of 1024 tokens\n\nA single paper (document) is divided in pages (token_splits): each page is an array of tokens.\n\nDocuments_tokenized contains the list of papers that must be summarized, and it is an array of documents.","metadata":{"id":"xXgv0sIltmeH"}},{"cell_type":"code","source":"max_size = 1024\nfor row in tqdm(paper_rows):\n    n_splits = math.ceil(len(row[\"input_ids\"][0])/max_size)\n    document_text_summary = \"\"\n    token_count = 0\n    for index in list(range(n_splits)):\n        if(index != n_splits-1):\n            # print(str(index*max_size) + \" - \" + str((index+1)*max_size))\n            token_splits = { \"input_ids\": torch.tensor(row['input_ids']['input_ids'][0][index*max_size:(index+1)*max_size]).unsqueeze(0).to(device),\n                                \"attention_mask\": torch.tensor(row['input_ids']['attention_mask'][0][index*max_size:(index+1)*max_size]).unsqueeze(0).to(device)}\n        else:\n            # print(str(index*max_size) + \" - \" + str(len(row['input_ids']['input_ids'][0])%max_size + index*max_size))\n            token_splits = { \"input_ids\": torch.tensor(row['input_ids']['input_ids'][0][index*max_size:len(row['input_ids']['input_ids'][0])%max_size + index*max_size]).unsqueeze(0).to(device),\n                            \"attention_mask\": torch.tensor(row['input_ids']['attention_mask'][0][index*max_size:len(row['input_ids']['input_ids'][0])%max_size + index*max_size]).unsqueeze(0).to(device)}\n\n        if(token_splits[\"input_ids\"].nelement() == 0):\n          continue\n        doc_token_summary = model.generate(input_ids=token_splits['input_ids'],\n                            attention_mask=token_splits['attention_mask'],\n                            min_length=16,\n                            max_length=64)\n        token_count += len(doc_token_summary[0])\n        extracted_summary = tokenizer.decode(doc_token_summary[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n\n        if (\".\" in extracted_summary):\n            document_text_summary += (\".\".join(extracted_summary.split(\".\")[0:-1])) + \"\\n\"\n        else:\n            document_text_summary += extracted_summary + \"\\n\"\n \n    \n    requests.post(f\"{host}/updateArticle\", json={\n      \"link\": row[\"link\"],\n      \"token_count\": token_count,\n      \"summary\": document_text_summary\n    })\n    \n    # row[\"summary\"] = document_text_summary\n    # row[\"input_ids\"] = None0","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":890},"id":"m5YHYROUtmeI","outputId":"bad5a935-56d5-4b17-e4c7-869c2acc44b7","trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":" 95%|█████████▌| 1459/1532 [2:04:03<05:56,  4.88s/it] ","output_type":"stream"}]}]}