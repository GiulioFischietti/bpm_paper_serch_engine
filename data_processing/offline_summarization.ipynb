{"cells":[{"cell_type":"markdown","metadata":{"id":"DxGk5T1ntmeD"},"source":["## Get all the articles to summarize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3QaK6LBtmeE","trusted":true},"outputs":[],"source":["import json\n","import requests\n","\n","host = \"https://0adf-82-50-143-221.ngrok-free.app\"\n","articles_json = requests.get(f\"{host}/articlesToSummarize\").json()"]},{"cell_type":"markdown","metadata":{"id":"sRL-nBuotmeE"},"source":["## Instanciate BART"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XaiIbmpItmeE","trusted":true},"outputs":[],"source":["from transformers import BartForConditionalGeneration, AutoTokenizer\n","\n","model_ckpt = \"sshleifer/distilbart-cnn-6-6\"\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","model = BartForConditionalGeneration.from_pretrained(model_ckpt).cuda()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"epITcYjKtmeF"},"source":["## Check Cuda comparibility"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kJVuXRpTtmeF","outputId":"6ee83d7a-99a4-4e52-992d-0b1b437f3c50","trusted":true},"outputs":[],"source":["import torch\n","from tqdm import tqdm\n","import math\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["# Tokenize and filter by number of pages\n","If a paper has more than 10 pages, it is filtered out as a single summary will be too long for chat gpt input\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uc8eLs1ptmeF","outputId":"04c74cd2-3fc6-4043-ad1d-ec5bf2fb1776","trusted":true},"outputs":[],"source":["page_size = 1024\n","\n","paper_rows = []\n","skipped_papers = []\n","for paper in tqdm(articles_json):\n","    tokens = tokenizer(paper['body'], padding='max_length', return_tensors='pt').to(device)\n","    n_pages = math.ceil(len(tokens[\"input_ids\"][0])/page_size)\n","    paper_rows.append(\n","            {\n","            \"link\": paper[\"link\"],\n","            \"body\":  paper[\"body\"],\n","            \"summary\":  \"\",\n","            \"input_ids\":  tokens\n","            }\n","        )"]},{"cell_type":"markdown","metadata":{"id":"xXgv0sIltmeH"},"source":["## Summarize Docs\n","Split tokens in pages of 1024 tokens\n","\n","A single paper (document) is divided in pages (token_splits): each page is an array of tokens.\n","\n","Documents_tokenized contains the list of papers that must be summarized, and it is an array of documents."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":890},"id":"m5YHYROUtmeI","outputId":"bad5a935-56d5-4b17-e4c7-869c2acc44b7","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":[" 95%|█████████▌| 1459/1532 [2:04:03<05:56,  4.88s/it] "]}],"source":["max_size = 1024\n","for row in tqdm(paper_rows):\n","    n_splits = math.ceil(len(row[\"input_ids\"][0])/max_size)\n","    document_text_summary = \"\"\n","    token_count = 0\n","    for index in list(range(n_splits)):\n","        if(index != n_splits-1):\n","            # print(str(index*max_size) + \" - \" + str((index+1)*max_size))\n","            token_splits = { \"input_ids\": torch.tensor(row['input_ids']['input_ids'][0][index*max_size:(index+1)*max_size]).unsqueeze(0).to(device),\n","                                \"attention_mask\": torch.tensor(row['input_ids']['attention_mask'][0][index*max_size:(index+1)*max_size]).unsqueeze(0).to(device)}\n","        else:\n","            # print(str(index*max_size) + \" - \" + str(len(row['input_ids']['input_ids'][0])%max_size + index*max_size))\n","            token_splits = { \"input_ids\": torch.tensor(row['input_ids']['input_ids'][0][index*max_size:len(row['input_ids']['input_ids'][0])%max_size + index*max_size]).unsqueeze(0).to(device),\n","                            \"attention_mask\": torch.tensor(row['input_ids']['attention_mask'][0][index*max_size:len(row['input_ids']['input_ids'][0])%max_size + index*max_size]).unsqueeze(0).to(device)}\n","\n","        if(token_splits[\"input_ids\"].nelement() == 0):\n","          continue\n","        doc_token_summary = model.generate(input_ids=token_splits['input_ids'],\n","                            attention_mask=token_splits['attention_mask'],\n","                            min_length=16,\n","                            max_length=64)\n","        token_count += len(doc_token_summary[0])\n","        extracted_summary = tokenizer.decode(doc_token_summary[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","\n","        if (\".\" in extracted_summary):\n","            document_text_summary += (\".\".join(extracted_summary.split(\".\")[0:-1])) + \"\\n\"\n","        else:\n","            document_text_summary += extracted_summary + \"\\n\"\n"," \n","    \n","    requests.post(f\"{host}/updateArticle\", json={\n","      \"link\": row[\"link\"],\n","      \"token_count\": token_count,\n","      \"summary\": document_text_summary\n","    })\n","    \n","    # row[\"summary\"] = document_text_summary\n","    # row[\"input_ids\"] = None0"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
