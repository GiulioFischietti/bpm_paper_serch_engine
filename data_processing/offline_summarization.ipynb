{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = MongoClient('localhost', 27017)\n",
    "db = mongo_client.scientific_articles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the articles to summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_json = list(db.articles.find({\"summary\": {\"$exists\": False}}))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load them into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0183eb58e3cd4bf3a7728967fff09444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for paper in tqdm(articles_json): \n",
    "    rows.append(\n",
    "        (\n",
    "            paper[\"link\"],\n",
    "            paper[\"body\"],\n",
    "            \"\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(data=rows, columns=[\"link\", \"body\", \"summary\"])\n",
    "articles_json = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciate BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"sshleifer/distilbart-cnn-6-6\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_ckpt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_ids = [tokenizer(doc, padding='max_length', return_tensors='pt').to(device) for doc in tqdm(df[\"body\"].to_list())]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split tokens in pages of 1024 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = 1024\n",
    "\n",
    "documents_tokenized = []\n",
    "\n",
    "for input_id in input_ids:\n",
    "    n_splits = math.ceil(len(input_id[0])/max_size)\n",
    "    token_splits = []\n",
    "    for index in list(range(n_splits)):\n",
    "        if(index != n_splits-1):\n",
    "            print(str(index*max_size) + \" - \" + str((index+1)*max_size))\n",
    "            token_splits.append({ \"input_ids\": torch.tensor(input_id['input_ids'][0][index*max_size:(index+1)*max_size]).unsqueeze(0), \n",
    "                                \"attention_mask\": torch.tensor(input_id['attention_mask'][0][index*max_size:(index+1)*max_size]).unsqueeze(0)})\n",
    "        else:\n",
    "            print(str(index*max_size) + \" - \" + str(len(input_id[0])%max_size + index*max_size))\n",
    "            token_splits.append({ \"input_ids\": torch.tensor(input_id['input_ids'][0][index*max_size:len(input_id[0])%max_size + index*max_size]).unsqueeze(0), \n",
    "                            \"attention_mask\": torch.tensor(input_id['attention_mask'][0][index*max_size:len(input_id[0])%max_size + index*max_size]).unsqueeze(0)})\n",
    "    \n",
    "    documents_tokenized.append(token_splits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize each page of 1024 tokens into 64 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "\n",
    "for doc_tokenized in tqdm(documents_tokenized):\n",
    "    doc_summary = []\n",
    "    for index in tqdm(list(range(len(doc_tokenized)))):\n",
    "        doc_summary.append(model.generate(input_ids=doc_tokenized[index]['input_ids'], \n",
    "                            attention_mask=doc_tokenized[index]['attention_mask'],\n",
    "                            min_length=16, \n",
    "                            max_length=64))\n",
    "    summaries.append(doc_summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the summaries obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summaries = []\n",
    "\n",
    "for summary in summaries:\n",
    "    text_summary = \"\"\n",
    "\n",
    "    for split in summary:\n",
    "        extracted_summary = tokenizer.decode(split[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    \n",
    "        if (\".\" in extracted_summary):\n",
    "            text_summary += (\".\".join(extracted_summary.split(\".\")[0:-1])) + \"\\n\"\n",
    "        else:\n",
    "            text_summary += extracted_summary + \"\\n\"\n",
    "    db.articles.updateOne({})\n",
    "    # load summary to mongodb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
