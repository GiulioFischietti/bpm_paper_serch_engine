{"cells":[{"cell_type":"markdown","metadata":{"id":"DxGk5T1ntmeD"},"source":["## Get all the articles to summarize"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-07-14T08:39:16.733306Z","iopub.status.busy":"2023-07-14T08:39:16.732873Z","iopub.status.idle":"2023-07-14T08:39:28.188390Z","shell.execute_reply":"2023-07-14T08:39:28.186353Z","shell.execute_reply.started":"2023-07-14T08:39:16.733273Z"},"id":"U3QaK6LBtmeE","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["789\n"]}],"source":["import requests\n","\n","host = \"http://144.24.201.133:5000\"\n","articles_json = requests.get(f\"{host}/articlesToSummarize?skip=1000\").json()\n","print(len(articles_json))"]},{"cell_type":"markdown","metadata":{"id":"sRL-nBuotmeE"},"source":["## Instanciate BART"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-07-14T08:39:28.190830Z","iopub.status.busy":"2023-07-14T08:39:28.190379Z","iopub.status.idle":"2023-07-14T08:39:33.387367Z","shell.execute_reply":"2023-07-14T08:39:33.386358Z","shell.execute_reply.started":"2023-07-14T08:39:28.190796Z"},"id":"XaiIbmpItmeE","trusted":true},"outputs":[],"source":["from transformers import BartForConditionalGeneration, AutoTokenizer\n","\n","model_ckpt = \"sshleifer/distilbart-cnn-6-6\"\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","model = BartForConditionalGeneration.from_pretrained(model_ckpt).cuda()"]},{"cell_type":"markdown","metadata":{"id":"epITcYjKtmeF"},"source":["## Check Cuda comparibility"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-07-14T08:39:33.389309Z","iopub.status.busy":"2023-07-14T08:39:33.388958Z","iopub.status.idle":"2023-07-14T08:39:33.395768Z","shell.execute_reply":"2023-07-14T08:39:33.394878Z","shell.execute_reply.started":"2023-07-14T08:39:33.389277Z"},"id":"kJVuXRpTtmeF","outputId":"6ee83d7a-99a4-4e52-992d-0b1b437f3c50","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import torch\n","from tqdm import tqdm\n","import math\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["# Tokenize and filter by number of pages\n","If a paper has more than 10 pages, it is filtered out as a single summary will be too long for chat gpt input\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-07-14T08:39:33.399199Z","iopub.status.busy":"2023-07-14T08:39:33.398480Z","iopub.status.idle":"2023-07-14T08:39:59.970343Z","shell.execute_reply":"2023-07-14T08:39:59.969416Z","shell.execute_reply.started":"2023-07-14T08:39:33.399118Z"},"id":"Uc8eLs1ptmeF","outputId":"04c74cd2-3fc6-4043-ad1d-ec5bf2fb1776","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 789/789 [00:33<00:00, 23.83it/s]\n"]}],"source":["page_size = 512\n","\n","paper_rows = []\n","skipped_papers = []\n","for paper in tqdm(articles_json):\n","    tokens = tokenizer(paper['body'], padding='max_length', return_tensors='pt').to(device)\n","    n_pages = math.ceil(len(tokens[\"input_ids\"][0])/page_size)\n","    paper_rows.append(\n","            {\n","            \"link\": paper[\"link\"],\n","            \"body\":  paper[\"body\"],\n","            \"summary\":  \"\",\n","            \"input_ids\":  tokens\n","            }\n","        )"]},{"cell_type":"markdown","metadata":{"id":"xXgv0sIltmeH"},"source":["## Summarize Docs\n","Split tokens in pages of 5 tokens\n","\n","A single paper (document) is divided in pages (token_splits): each page is an array of tokens.\n","\n","Documents_tokenized contains the list of papers that must be summarized, and it is an array of documents."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":890},"execution":{"iopub.execute_input":"2023-07-14T08:39:59.972642Z","iopub.status.busy":"2023-07-14T08:39:59.971991Z"},"id":"m5YHYROUtmeI","outputId":"bad5a935-56d5-4b17-e4c7-869c2acc44b7","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":[" 58%|█████▊    | 460/789 [5:08:24<3:40:35, 40.23s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[6], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mif\u001b[39;00m(token_splits[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mnelement() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m     16\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m doc_token_summary \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49mtoken_splits[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     18\u001b[0m                     attention_mask\u001b[39m=\u001b[39;49mtoken_splits[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     19\u001b[0m                     min_length\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[0;32m     20\u001b[0m                     max_length\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m)\n\u001b[0;32m     21\u001b[0m token_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(doc_token_summary[\u001b[39m0\u001b[39m])\n\u001b[0;32m     22\u001b[0m extracted_summary \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(doc_token_summary[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n","File \u001b[1;32mc:\\Users\\fisch\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\fisch\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1611\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1604\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1605\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1606\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   1607\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1608\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1609\u001b[0m     )\n\u001b[0;32m   1610\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[1;32m-> 1611\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_search(\n\u001b[0;32m   1612\u001b[0m         input_ids,\n\u001b[0;32m   1613\u001b[0m         beam_scorer,\n\u001b[0;32m   1614\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1615\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1616\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1617\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1618\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1619\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1620\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1621\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1622\u001b[0m     )\n\u001b[0;32m   1624\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[0;32m   1625\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1626\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n","File \u001b[1;32mc:\\Users\\fisch\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:2928\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2923\u001b[0m next_token_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madjust_logits_during_generation(next_token_logits, cur_len\u001b[39m=\u001b[39mcur_len)\n\u001b[0;32m   2924\u001b[0m next_token_scores \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlog_softmax(\n\u001b[0;32m   2925\u001b[0m     next_token_logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m   2926\u001b[0m )  \u001b[39m# (batch_size * num_beams, vocab_size)\u001b[39;00m\n\u001b[1;32m-> 2928\u001b[0m next_token_scores_processed \u001b[39m=\u001b[39m logits_processor(input_ids, next_token_scores)\n\u001b[0;32m   2929\u001b[0m next_token_scores \u001b[39m=\u001b[39m next_token_scores_processed \u001b[39m+\u001b[39m beam_scores[:, \u001b[39mNone\u001b[39;00m]\u001b[39m.\u001b[39mexpand_as(next_token_scores)\n\u001b[0;32m   2931\u001b[0m \u001b[39m# Store scores, attentions and hidden_states when required\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\fisch\\anaconda3\\lib\\site-packages\\transformers\\generation\\logits_process.py:92\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[1;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     91\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores)\n\u001b[0;32m     93\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n","File \u001b[1;32mc:\\Users\\fisch\\anaconda3\\lib\\site-packages\\transformers\\generation\\logits_process.py:492\u001b[0m, in \u001b[0;36mNoRepeatNGramLogitsProcessor.__call__\u001b[1;34m(self, input_ids, scores)\u001b[0m\n\u001b[0;32m    490\u001b[0m num_batch_hypotheses \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    491\u001b[0m cur_len \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m--> 492\u001b[0m banned_batch_tokens \u001b[39m=\u001b[39m _calc_banned_ngram_tokens(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mngram_size, input_ids, num_batch_hypotheses, cur_len)\n\u001b[0;32m    494\u001b[0m \u001b[39mfor\u001b[39;00m i, banned_tokens \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(banned_batch_tokens):\n\u001b[0;32m    495\u001b[0m     scores[i, banned_tokens] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minf\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\fisch\\anaconda3\\lib\\site-packages\\transformers\\generation\\logits_process.py:465\u001b[0m, in \u001b[0;36m_calc_banned_ngram_tokens\u001b[1;34m(ngram_size, prev_input_ids, num_hypos, cur_len)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39mif\u001b[39;00m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m<\u001b[39m ngram_size:\n\u001b[0;32m    462\u001b[0m     \u001b[39m# return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\u001b[39;00m\n\u001b[0;32m    463\u001b[0m     \u001b[39mreturn\u001b[39;00m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos)]\n\u001b[1;32m--> 465\u001b[0m generated_ngrams \u001b[39m=\u001b[39m _get_ngrams(ngram_size, prev_input_ids, num_hypos)\n\u001b[0;32m    467\u001b[0m banned_tokens \u001b[39m=\u001b[39m [\n\u001b[0;32m    468\u001b[0m     _get_generated_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n\u001b[0;32m    469\u001b[0m     \u001b[39mfor\u001b[39;00m hypo_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos)\n\u001b[0;32m    470\u001b[0m ]\n\u001b[0;32m    471\u001b[0m \u001b[39mreturn\u001b[39;00m banned_tokens\n","File \u001b[1;32mc:\\Users\\fisch\\anaconda3\\lib\\site-packages\\transformers\\generation\\logits_process.py:442\u001b[0m, in \u001b[0;36m_get_ngrams\u001b[1;34m(ngram_size, prev_input_ids, num_hypos)\u001b[0m\n\u001b[0;32m    440\u001b[0m generated_ngrams \u001b[39m=\u001b[39m [{} \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos)]\n\u001b[0;32m    441\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_hypos):\n\u001b[1;32m--> 442\u001b[0m     gen_tokens \u001b[39m=\u001b[39m prev_input_ids[idx]\u001b[39m.\u001b[39;49mtolist()\n\u001b[0;32m    443\u001b[0m     generated_ngram \u001b[39m=\u001b[39m generated_ngrams[idx]\n\u001b[0;32m    444\u001b[0m     \u001b[39mfor\u001b[39;00m ngram \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[gen_tokens[i:] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ngram_size)]):\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["max_size = 512\n","for row in tqdm(paper_rows):\n","    n_splits = math.ceil(len(row[\"input_ids\"][0])/max_size)\n","    document_text_summary = \"\"\n","    token_count = 0\n","    for index in list(range(n_splits)):\n","        if(index != n_splits-1):\n","            token_splits = { \"input_ids\": torch.tensor(row['input_ids']['input_ids'][0][index*max_size:(index+1)*max_size]).unsqueeze(0).to(device),\n","                                \"attention_mask\": torch.tensor(row['input_ids']['attention_mask'][0][index*max_size:(index+1)*max_size]).unsqueeze(0).to(device)}\n","\n","        else:\n","            token_splits = { \"input_ids\": torch.tensor(row['input_ids']['input_ids'][0][index*max_size:len(row['input_ids']['input_ids'][0])%max_size + index*max_size]).unsqueeze(0).to(device),\n","                            \"attention_mask\": torch.tensor(row['input_ids']['attention_mask'][0][index*max_size:len(row['input_ids']['input_ids'][0])%max_size + index*max_size]).unsqueeze(0).to(device)}\n","\n","        if(token_splits[\"input_ids\"].nelement() == 0):\n","          continue\n","        doc_token_summary = model.generate(input_ids=token_splits['input_ids'],\n","                            attention_mask=token_splits['attention_mask'],\n","                            min_length=16,\n","                            max_length=128)\n","        token_count += len(doc_token_summary[0])\n","        extracted_summary = tokenizer.decode(doc_token_summary[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","        \n","        \n","        if (\".\" in extracted_summary):\n","             document_text_summary += (\".\".join(extracted_summary.split(\".\")[0:-1])) + \".\\n\"\n","        else:\n","            document_text_summary += extracted_summary + \".\\n\"\n","    requests.post(f\"{host}/updateArticle\", json={\n","      \"link\": row[\"link\"],\n","      \"token_count\": token_count,\n","      \"summary\": document_text_summary.replace(\"..\", \".\")\n","    }) \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
