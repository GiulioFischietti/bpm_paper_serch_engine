{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect on MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.scientific_articles\n",
    "articles_collection = db.articles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_data = list(articles_collection.find({\"sentences\": {\"$exists\": False}}))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciate BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "model_ckpt = \"sshleifer/distilbart-cnn-6-6\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_ckpt)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Extraction\n",
    "\n",
    "For each article, Tokenize, Extract and Update the corresponding document on MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bson import ObjectId\n",
    "\n",
    "max_size = 1024\n",
    "\n",
    "for article_json in tqdm(articles_data, desc=\"Elaborating all articles\"):\n",
    "\n",
    "    input_id = tokenizer(article_json['body'], padding='max_length', return_tensors='pt').to(device)\n",
    "    n_splits = math.ceil(len(input_id[0])/max_size)\n",
    "    if(n_splits<5 and n_splits>1):\n",
    "        print(f\"n splits: {n_splits}\")\n",
    "        token_splits = []\n",
    "        doc_summary_splits = []\n",
    "        doc_summary_sentences = []\n",
    "        \n",
    "        # for each split of document, tokenize it and append into token_splits\n",
    "        for index in list(range(n_splits)):\n",
    "            if(index != n_splits-1):\n",
    "                print(str(index*max_size) + \" - \" + str((index+1)*max_size))\n",
    "                token_splits.append({ \"input_ids\": torch.tensor(input_id['input_ids'][0][index*max_size:(index+1)*max_size]).unsqueeze(0), \n",
    "                                    \"attention_mask\": torch.tensor(input_id['attention_mask'][0][index*max_size:(index+1)*max_size]).unsqueeze(0)})\n",
    "            else:\n",
    "                print(str(index*max_size) + \" - \" + str(len(input_id[0])%max_size + index*max_size))\n",
    "                token_splits.append({ \"input_ids\": torch.tensor(input_id['input_ids'][0][index*max_size:len(input_id[0])%max_size + index*max_size]).unsqueeze(0), \n",
    "                                \"attention_mask\": torch.tensor(input_id['attention_mask'][0][index*max_size:len(input_id[0])%max_size + index*max_size]).unsqueeze(0)})\n",
    "        \n",
    "        # for each token split, generate token array summary \n",
    "        for token_split in tqdm(token_splits, desc=\"generating summary...\"):\n",
    "            doc_summary_splits.append(model.generate(input_ids=token_split['input_ids'], \n",
    "                                attention_mask=token_split['attention_mask'],\n",
    "                                max_length=96))\n",
    "            \n",
    "        \n",
    "        # for each token summary of document, decode it into text\n",
    "        for doc_summary_split in doc_summary_splits:\n",
    "            extracted_summary = tokenizer.decode(doc_summary_split[0], skip_special_tokens=True, clean_up_tokenization_spaces=True) + '\\n'\n",
    "            if (\".\" in extracted_summary):\n",
    "                doc_summary_sentences.append(\".\".join(extracted_summary.split(\".\")[0:-1]))\n",
    "            else:\n",
    "                doc_summary_sentences.append(extracted_summary)\n",
    "        \n",
    "        articles_collection.update_one({\"_id\": ObjectId(article_json['_id'])}, {\"$set\": {\"sentences\": doc_summary_sentences}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x19c65ec3b50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bson import ObjectId\n",
    "articles_collection.update_one({\"_id\": ObjectId(article_json['_id'])}, {\"$set\": {\"sentences\": [\"\"]}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
